{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 read train and testb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def count_one_ratio(arr):\n",
    "    counts = arr.value_counts()\n",
    "    try:\n",
    "        if counts.sum()<100:\n",
    "            return 0\n",
    "        return counts[1]/counts.sum()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "train_data = pd.read_csv('../data/full_size/atec_anti_fraud_train.csv',index_col = 0)\n",
    "testb_data = pd.read_csv('../data/full_size/atec_anti_fraud_test_b.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 split data into labeled and unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data=(train_data[train_data['label']==-1])\n",
    "normal_data = train_data[train_data['label']==0]\n",
    "anormal_data=train_data[train_data['label']==1]\n",
    "labeled_data = train_data[(train_data['label']==0)|(train_data['label']==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = int(labeled_data.shape[0]*0.8)\n",
    "train_x = labeled_data.iloc[:train_num,:].drop(columns=['label'])\n",
    "train_y = labeled_data.iloc[:train_num,:]['label']\n",
    "test_x = labeled_data.iloc[train_num:,:].drop(columns=['label'])\n",
    "test_y = labeled_data.iloc[train_num:,:]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. generate risk ratio of every feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_ratios = {} # key:feature_name,value: list of anormal probabilities for every possible value\n",
    "for feature in labeled_data.columns[2:]:\n",
    "    max_value = labeled_data[feature].max()\n",
    "    min_value = labeled_data[feature].min()\n",
    "    value_num = labeled_data[feature].value_counts().shape[0]\n",
    "    if value_num>10000:\n",
    "        # guarantee the number in every bin won't be too small\n",
    "        bins = 1000\n",
    "        ratio = labeled_data.groupby(pd.cut(labeled_data[feature],np.linspace(min_value-0.1,max_value,bins))).label.mean()\n",
    "        risk_ratios[feature]=ratio\n",
    "    else:\n",
    "        risk_ratios[feature]=labeled_data.groupby(feature).label.agg(count_one_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 get high risk feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risks = {}\n",
    "for feature,ratios in risk_ratios.items():\n",
    "    high_risk = ratios[ratios>0.35]\n",
    "    if not high_risk.empty:\n",
    "        high_risks[feature]=high_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_risks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(high_risks['f82'].index,pd.core.indexes.category.CategoricalIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_risks['f82'].index.get_loc(76000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. prediction and evaluation on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 define predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannual_predict(X):\n",
    "    '''\n",
    "    X should be in shape of (n_samples,n_columns)\n",
    "    '''\n",
    "    risk_thr = 0.5\n",
    "    Y = np.zeros(X.shape[0])\n",
    "    Y_proba = np.zeros(X.shape[0])\n",
    "    i = 0\n",
    "    all_risk = []\n",
    "    for x in X.iterrows():\n",
    "        x=x[1]\n",
    "        risks = []\n",
    "        risk = 0\n",
    "        for feature_name,feature_ratio in high_risks.items():\n",
    "            value = x[feature_name]\n",
    "            if value in feature_ratio:\n",
    "                risks.append(feature_ratio[value])\n",
    "            else:\n",
    "                if isinstance(feature_ratio.index,pd.core.indexes.category.CategoricalIndex):\n",
    "                    try:\n",
    "                        loc = feature_ratio.index.get_loc(value)\n",
    "                        risks.append(feature_ratio[loc])\n",
    "                    except:\n",
    "                        pass\n",
    "        if risks:\n",
    "            risk_score = np.array(risks).mean()\n",
    "            Y_proba[i] = risk_score\n",
    "            if risk_score>risk_thr:\n",
    "                Y[i] = 1\n",
    "        i+=1\n",
    "    return Y,Y_proba\n",
    "\n",
    "# def mannual_predict_proba(X):\n",
    "#     '''\n",
    "#     X should be in shape of (n_samples,n_columns)\n",
    "#     '''\n",
    "#     Y = np.zeros(X.shape[0])\n",
    "#     i = 0\n",
    "#     all_risk = []\n",
    "#     for x in X.iterrows():\n",
    "#         print(i)\n",
    "#         x=x[1]\n",
    "#         risks = []\n",
    "#         risk = 0\n",
    "#         for feature_name,feature_ratio in high_risks.items():\n",
    "#             value = x[feature_name]\n",
    "#             if value in feature_ratio:\n",
    "#                 risks.append(feature_ratio[value])\n",
    "#         if risks:\n",
    "#             risk_score = np.array(risks).mean()\n",
    "#             Y[i]=risk_score\n",
    "#         i+=1\n",
    "#     return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y,predict_y_proba = mannual_predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import bisect\n",
    "def get_tpr_from_fpr(fpr_array, tpr_array, target):\n",
    "    fpr_index = np.where(fpr_array == target)\n",
    "    assert target <= 0.01, 'the value of fpr in the custom metric function need lt 0.01'\n",
    "    if len(fpr_index[0]) > 0:\n",
    "        return np.mean(tpr_array[fpr_index])\n",
    "    else:\n",
    "        tmp_index = bisect.bisect(fpr_array, target)\n",
    "        fpr_tmp_1 = fpr_array[tmp_index-1]\n",
    "        fpr_tmp_2 = fpr_array[tmp_index]\n",
    "        if (target - fpr_tmp_1) > (fpr_tmp_2 - target):\n",
    "            tpr_index = tmp_index\n",
    "        else:\n",
    "            tpr_index = tmp_index - 1\n",
    "        return tpr_array[tpr_index]\n",
    "\n",
    "\n",
    "def eval_metric(labels,pred):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, pred, pos_label=1)\n",
    "    tpr1 = get_tpr_from_fpr(fpr, tpr, 0.001)\n",
    "    tpr2 = get_tpr_from_fpr(fpr, tpr, 0.005)\n",
    "    tpr3 = get_tpr_from_fpr(fpr, tpr, 0.01)\n",
    "    return 0.4*tpr1 + 0.3*tpr2 + 0.3*tpr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6020408163265306 0.02425986842105263 0.02425986842105263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(precision_score(test_y,predict_y),recall_score(test_y,predict_y),eval_metric(test_y,predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = (predict_y_proba>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    197904\n",
       "0.619792        31\n",
       "0.765766        15\n",
       "0.624334         7\n",
       "0.673267         7\n",
       "0.641557         5\n",
       "0.640645         4\n",
       "0.611570         4\n",
       "0.646018         4\n",
       "0.692779         4\n",
       "0.643618         3\n",
       "0.631562         2\n",
       "0.642419         2\n",
       "0.637097         2\n",
       "0.641988         2\n",
       "0.628794         2\n",
       "0.659643         1\n",
       "0.634876         1\n",
       "0.625793         1\n",
       "0.652127         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predict_y_proba).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-8b1e30562baf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_y_proba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "predict_y_proba.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
