{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divid date\n",
    "import datetime\n",
    "def date2weekday(date):\n",
    "    date = str(date)\n",
    "    year = int(date[0:4])\n",
    "    month = int(date[4:6])\n",
    "    day = int(date[6:])\n",
    "    return datetime.datetime(year,month,day).weekday()\n",
    "\n",
    "def ant_score(truth,score):\n",
    "    FNR1 = 0.001\n",
    "    FNR2 = 0.005\n",
    "    FNR3 = 0.01\n",
    "    min1 = min2 = min3 = 1\n",
    "    for thr in np.arange(0,1+0.001,0.001):\n",
    "        evaluate_table = pd.DataFrame({'truth':truth,'score':score})\n",
    "        evaluate_table.loc[evaluate_table['score']>=thr,'score']=1\n",
    "        evaluate_table.loc[evaluate_table['score']<thr,'score']=0\n",
    "        TP = evaluate_table.loc[(evaluate_table['score']==1)&(evaluate_table['truth']==1)].shape[0]\n",
    "        FN = evaluate_table.loc[(evaluate_table['score']==0)&(evaluate_table['truth']==1)].shape[0]\n",
    "        TN = evaluate_table.loc[(evaluate_table['score']==0)&(evaluate_table['truth']==0)].shape[0]\n",
    "        FP = evaluate_table.loc[(evaluate_table['score']==1)&(evaluate_table['truth']==0)].shape[0]\n",
    "        TPR = TP/(TP+FN)\n",
    "        FNR = FP/(TN+FP)\n",
    "        if abs(FNR-FNR1)<min1:\n",
    "            min1 = abs(FNR-FNR1)\n",
    "            FNR11 = FNR\n",
    "            TPR1 = TPR\n",
    "        if abs(FNR-FNR2)<min2:\n",
    "            min2 = abs(FNR-FNR2)\n",
    "            FNR22 = FNR\n",
    "            TPR2 = TPR\n",
    "        if abs(FNR-FNR3)<min3:\n",
    "            min3 = abs(FNR-FNR3)\n",
    "            FNR33 = FNR\n",
    "            TPR3 = TPR\n",
    "    return 0.4*TPR1+0.3*TPR2+0.3*TPR3\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import bisect\n",
    "\n",
    "\n",
    "def get_tpr_from_fpr(fpr_array, tpr_array, target):\n",
    "    fpr_index = np.where(fpr_array == target)\n",
    "    assert target <= 0.01, 'the value of fpr in the custom metric function need lt 0.01'\n",
    "    if len(fpr_index[0]) > 0:\n",
    "        return np.mean(tpr_array[fpr_index])\n",
    "    else:\n",
    "        tmp_index = bisect.bisect(fpr_array, target)\n",
    "        fpr_tmp_1 = fpr_array[tmp_index-1]\n",
    "        fpr_tmp_2 = fpr_array[tmp_index]\n",
    "        if (target - fpr_tmp_1) > (fpr_tmp_2 - target):\n",
    "            tpr_index = tmp_index\n",
    "        else:\n",
    "            tpr_index = tmp_index - 1\n",
    "        return tpr_array[tpr_index]\n",
    "\n",
    "\n",
    "def eval_metric(labels,pred):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, pred, pos_label=1)\n",
    "    tpr1 = get_tpr_from_fpr(fpr, tpr, 0.001)\n",
    "    tpr2 = get_tpr_from_fpr(fpr, tpr, 0.005)\n",
    "    tpr3 = get_tpr_from_fpr(fpr, tpr, 0.01)\n",
    "    return 0.4*tpr1 + 0.3*tpr2 + 0.3*tpr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant definition\n",
    "# small_data_path = './data/small_size/sample_atec_anti_fraud_train.csv'\n",
    "train_path = '../data/full_size/atec_anti_fraud_train.csv'\n",
    "testb_path='../data/full_size/atec_anti_fraud_test_b.csv'\n",
    "train_data = pd.read_csv(train_path,index_col = 0)\n",
    "testb_data = pd.read_csv('../data/full_size/atec_anti_fraud_test_b.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find no missing value features\n",
    "no_nan_features = ['date']\n",
    "i = 1\n",
    "while i<len(testb_data.columns):\n",
    "    name = testb_data.columns[i]\n",
    "    if train_data[name].isnull().sum()==0:\n",
    "        no_nan_features.append(name)\n",
    "    i+=1\n",
    "    \n",
    "# find small missing features\n",
    "small_missing_features=[]\n",
    "i = 1\n",
    "while i<len(testb_data.columns):\n",
    "    name = testb_data.columns[i]\n",
    "    train_missing_rate = train_data[name].isnull().sum()/train_data.shape[0]\n",
    "    test_missing_rate = testb_data[name].isnull().sum()/testb_data.shape[0]\n",
    "    if 0<train_missing_rate<0.3 and abs(test_missing_rate-train_missing_rate)<0.1:\n",
    "        small_missing_features.append(name)\n",
    "    i+=1\n",
    "    \n",
    "filldable_features = small_missing_features+no_nan_features\n",
    "# feature selection\n",
    "feature_score_files=['xgb_feature_scores.csv','lgb_feature_scores2.csv']\n",
    "common_important_features=set()\n",
    "all_important_features=set()\n",
    "top=100\n",
    "for file in feature_score_files:\n",
    "    features = set(pd.read_csv(file,index_col = 0,header=None).sort_values(by=1,ascending=False).iloc[:top,0].index.tolist())\n",
    "    all_important_features = all_important_features|features\n",
    "    if common_important_features:\n",
    "        common_important_features = common_important_features&features\n",
    "    else:\n",
    "        common_important_features=features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 fill missing values for fiildable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[filldable_features] = train_data[filldable_features].fillna(train_data[filldable_features].mean())\n",
    "testb_data[filldable_features] = testb_data[filldable_features].fillna(testb_data[filldable_features].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 eliminate unlabeled data/convert -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert -1 to 1\n",
    "train_data['label'] = train_data['label'].apply(lambda x: 1 if x==-1 else x)\n",
    "train_data = train_data.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "def random_subsample(data,target_ratio):\n",
    "    pos_data = data[data['label']==0]\n",
    "    neg_data = data[data['label']==1]\n",
    "    target_pos_num = int(neg_data.shape[0]/target_ratio)\n",
    "    pos_data = data.iloc[np.random.randint(pos_data.shape[0],size=target_pos_num),:]\n",
    "    return pd.concat([pos_data,neg_data])\n",
    "    \n",
    "def random_oversample(data,target_ratio):\n",
    "    pos_data = data[data['label']==0]\n",
    "    neg_data = data[data['label']==1]\n",
    "    target_new_neg_num = int(pos_data.shape[0]*target_ratio)-neg_data.shape[0]\n",
    "    new_neg_data = neg_data.iloc[np.random.randint(neg_data.shape[0],size=target_new_neg_num),:]\n",
    "    neg_data = pd.concat([neg_data,new_neg_data])\n",
    "    \n",
    "    return pd.concat([pos_data,neg_data])\n",
    "    \n",
    "def mySMOTE(data):\n",
    "    X = data.drop(columns=['label'])\n",
    "    Y = data.label\n",
    "    resampled_x,resampled_y = SMOTE().fit_sample(X,Y)\n",
    "    resampled_data = resasmpled_x.copy()\n",
    "    resampled_data['label'] = resampled_y\n",
    "    return resampled_data\n",
    "    \n",
    "def myADASYN(data):\n",
    "    X = data.drop(columns=['label'])\n",
    "    Y = data.label\n",
    "    resampled_x,resampled_y = ADASYN().fit_sample(X,Y)\n",
    "    resampled_data = resasmpled_x.copy()\n",
    "    resampled_data['label'] = resampled_y\n",
    "    return resampled_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # strategy 1: only filldable\n",
    "# train_data = train_data[['label']+filldable_features]\n",
    "\n",
    "# stragey 2: filldable | common_important:158\n",
    "selected_features = list(set(filldable_features)|common_important_features)\n",
    "selected_features = testb_data.columns.tolist()\n",
    "train_data = train_data[['label']+selected_features]\n",
    "\n",
    "# # strategy 3: filldable|all_important\n",
    "# selected_features = list(set(filldable_features)|all_important_features)\n",
    "# train_data = train_data[['label']+selected_features]\n",
    "\n",
    "\n",
    "train_num = int(0.8*train_data.shape[0])\n",
    "test_data = train_data.iloc[train_num:,:]\n",
    "cut_train_data = train_data.iloc[:train_num,:]\n",
    "sampled_train_data = cut_train_data.copy()\n",
    "# sampled_train_data = random_oversample(sampled_train_data,0.05)\n",
    "train_x = sampled_train_data.drop(columns=['label'])\n",
    "train_y = sampled_train_data['label']\n",
    "test_x = test_data.drop(columns=['label'])\n",
    "test_y = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build LightGMB model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Grid Search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':[3,5,7,9],'num_leaves':[6,25,85,350], 'num_trees':[100,300,500,700,900]}\n",
    "lgb_clf = lgb.LGBMClassifier(boosting_type= 'gbdt', objective='binary',colsample_bytree=0.8,\n",
    "                   subsample= 0.8)\n",
    "lgb_random_cv = RandomizedSearchCV(lgb_clf,param,verbose=True,scoring='precision')\n",
    "lgb_random_cv.fit(train_x,train_y,eval_metric='error',eval_set=[(test_x, test_y)],early_stopping_rounds=20)\n",
    "lgb_random_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 train lightgbm based on best parameters with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kfold_by_date(data,k=5):\n",
    "    kf = KFold(n_splits=k,random_state=0,shuffle=True)\n",
    "    dates = np.array(data['date'].value_counts().index.tolist())\n",
    "    for train_index,test_index in kf.split(dates):\n",
    "        train_data = data[data['date'].isin(dates[train_index])]\n",
    "        valid_data = data[data['date'].isin(dates[test_index])]\n",
    "        yield train_data,valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 normal k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train best parameter lgb_random_cv\n",
    "neg_num = sampled_train_data.label.value_counts()[0]\n",
    "pos_num = sampled_train_data.label.value_counts()[0]\n",
    "lgb_300n350le9d01l = lgb.LGBMClassifier(boosting_type= 'dart', objective='binary',colsample_bytree=0.6,\n",
    "                   subsample= 0.6,max_depth=9,num_leaves=400,n_estimators=200,random_state = 0,scale_pos_weight=neg_num/pos_num)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "kf = KFold(n_splits=5,random_state=0,shuffle=True)\n",
    "lgb_300n350le9d01l_list = []\n",
    "predict_probas = []\n",
    "i=1\n",
    "for train_index,test_index in kf.split(sampled_train_data):\n",
    "    sub_train_x = train_x.iloc[train_index]\n",
    "    sub_train_y = train_y.iloc[train_index]\n",
    "    sub_test_x = train_x.iloc[test_index]\n",
    "    sub_test_y = train_y.iloc[test_index]\n",
    "    \n",
    "    # training\n",
    "    print('fitting model{}...'.format(i))\n",
    "    lgb_300n350le9d01l.fit(sub_train_x,sub_train_y,eval_metric='error',eval_set=[(test_x, test_y)],early_stopping_rounds=100)\n",
    "    print('model{} predicting...'.format(i))\n",
    "    sub_predict_y = lgb_300n350le9d01l.predict(sub_test_x)\n",
    "    sub_predict_y_proba = lgb_300n350le9d01l.predict_proba(sub_test_x)[:,1]\n",
    "    print('sub scoring...')\n",
    "    print('precision: {},recall:{},ant_score:{}'.format(metrics.precision_score(sub_test_y,sub_predict_y),\n",
    "                                                           metrics.recall_score(sub_test_y,sub_predict_y),\n",
    "                                                           eval_metric(sub_test_y,sub_predict_y)))\n",
    "    print('out scoring...')\n",
    "    predict_y = lgb_300n350le9d01l.predict(test_x)\n",
    "    predict_y_proba = lgb_300n350le9d01l.predict_proba(test_x)[:,1]\n",
    "    print('precision: {}, recall: {}, ant_score: {}'.format(metrics.precision_score(test_y,predict_y),\n",
    "                                                       metrics.recall_score(test_y,predict_y),\n",
    "                                                       eval_metric(test_y,predict_y)))\n",
    "    predict_probas.append(predict_y_proba)\n",
    "    lgb_300n350le9d01l_list.append(lgb_300n350le9d01l)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 date-based kfold-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train best parameter lgb_random_cv\n",
    "\n",
    "lgb_300n350le9d01l = lgb.LGBMClassifier(boosting_type= 'dart', objective='binary',colsample_bytree=0.8,\n",
    "                   subsample= 0.8,max_depth=10,num_leaves=800,n_estimators=100)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "kf = KFold(n_splits=5,random_state=0,shuffle=True)\n",
    "lgb_300n350le9d01l_list = []\n",
    "predict_probas = []\n",
    "i=1\n",
    "for sub_train_data,sub_test_data in kfold_by_date(sampled_train_data,k=5):\n",
    "    sub_train_x = sub_train_data.drop(columns=['label'])\n",
    "    sub_train_y = sub_train_data['label']\n",
    "    sub_test_x = sub_test_data.drop(columns=['label'])\n",
    "    sub_test_y = sub_test_data['label']\n",
    "    # training\n",
    "    print('fitting model{}...'.format(i))\n",
    "    lgb_300n350le9d01l.fit(sub_train_x,sub_train_y,eval_metric='error',eval_set=[(test_x, test_y)],early_stopping_rounds=50)\n",
    "    print('model{} predicting...'.format(i))\n",
    "    sub_predict_y = lgb_300n350le9d01l.predict(sub_test_x)\n",
    "    sub_predict_y_proba = lgb_300n350le9d01l.predict_proba(sub_test_x)[:,1]\n",
    "    print('sub scoring...')\n",
    "    print('precision: {},recall:{},ant_score:{}'.format(metrics.precision_score(sub_test_y,sub_predict_y),\n",
    "                                                           metrics.recall_score(sub_test_y,sub_predict_y),\n",
    "                                                           eval_metric(sub_test_y,sub_predict_y)))\n",
    "    print('out scoring...')\n",
    "    predict_y = lgb_300n350le9d01l.predict(test_x)\n",
    "    predict_y_proba = lgb_300n350le9d01l.predict_proba(test_x)[:,1]\n",
    "    print('precision: {}, recall: {}, ant_score: {}'.format(metrics.precision_score(test_y,predict_y),\n",
    "                                                       metrics.recall_score(test_y,predict_y),\n",
    "                                                       eval_metric(test_y,predict_y)))\n",
    "    predict_probas.append(predict_y_proba)\n",
    "    lgb_300n350le9d01l_list.append(lgb_300n350le9d01l)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_300n350le9d01l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probas = pd.DataFrame(predict_probas).T\n",
    "predict_scores = predict_probas.mean(axis = 1).values\n",
    "print('ant_score:{}'.format(eval_metric(test_y,predict_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 prediction on testb set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict on testb\n",
    "testb_data = pd.read_csv('../data/full_size/atec_anti_fraud_test_b.csv',index_col = 0)\n",
    "\n",
    "# predict\n",
    "testb_data[filldable_features] = testb_data[filldable_features].fillna(testb_data[filldable_features].mean())\n",
    "testb_data = testb_data[selected_features]\n",
    "# testb_data['date'] = testb_data['date'].apply(lambda x:int(str(x)[6:]))\n",
    "print('predicting on final outer testset.....')\n",
    "scores = [] # store score predicted by every cv model\n",
    "for model in lgb_300n350le9d01l_list:\n",
    "    score = model.predict_proba(testb_data)[:,1]\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = pd.DataFrame(scores).T.mean(axis=1).values\n",
    "print(final_scores.mean(),final_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('writing result to file...')\n",
    "final_result = pd.DataFrame({'score':final_scores},index=testb_data.index)\n",
    "final_result.to_csv('../submission/testb_lgb_500n400le9d01l_fillnan_sf158_normalcv5_original_date_scale_pos_weight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
